{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ==================================================================================================\n",
    "# import packages\n",
    "from pathlib import Path\n",
    "import torch as th\n",
    "from torch.utils.data import DataLoader\n",
    "import time\n",
    "from tqdm import tqdm # Instantly make your loops show a smart progress meter\n",
    "import os\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchsummary import summary\n",
    "from PIL import Image\n",
    "import uuid\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Import other scripts here \n",
    "from drought_impact_dataset import *\n",
    "from drought_impact_sampler import *\n",
    "# Import a config file for training \n",
    "from utils.jura_config_rec_2020_test import * #train_dataset_params, train_sampler_params, test_dataset_params, test_sampler_params, sim_params, model_params\n",
    "from torch.utils.data import DataLoader\n",
    "from utils.utils_pixel import *\n",
    "from model import *\n",
    "\n",
    "# MLFlow\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "import logging\n",
    "logging.basicConfig(level=logging.WARN)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "#th.manual_seed(0)\n",
    "#random.seed(0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took 0.00034165382385253906 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "# ==================================================================================================\n",
    "# Get Dataset details from configuration file\n",
    "s2_path =  Path(train_dataset_params['s2_path'])\n",
    "era_path =  Path(train_dataset_params['era_path'])\n",
    "dem_path =   Path(train_dataset_params['dem_path'])\n",
    "env_path =  Path(train_dataset_params['env_path'])\n",
    "ts_delta =  train_dataset_params['ts_delta']\n",
    "ts_len =  train_dataset_params['ts_len']\n",
    "len_preds =  train_dataset_params['len_preds']\n",
    "focus_time_train = train_dataset_params['focus_time']\n",
    "focus_list_train = train_dataset_params['focus_list']\n",
    "ratio = train_dataset_params['ratio']\n",
    "data_file_extension = train_dataset_params['data_file_extension'] \n",
    "feature_set = train_dataset_params['feature_set']\n",
    "remove_bands = train_dataset_params['remove_bands']\n",
    "agg_funct_dict = train_dataset_params['agg_funct_dict']\n",
    "multiple_labels = train_dataset_params['multiple_labels']\n",
    "\"\"\"\n",
    "focus_time_val = val_dataset_params['focus_time']\n",
    "focus_list_val = val_dataset_params['focus_list']\n",
    "\"\"\"\n",
    "focus_list_test = test_dataset_params['focus_list']\n",
    "focus_time_test = test_dataset_params['focus_time']\n",
    "#focus_time_test2 = test2_dataset_params['focus_time']\n",
    "\n",
    "end_time = time.time()\n",
    "print(f'Took {end_time-start_time} seconds')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took 0.0001773834228515625 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "# ==================================================================================================\n",
    "# Get Sampler details from configuration file\n",
    "batch_size_tr =  train_sampler_params['batch_size'] \n",
    "n_batch = sim_params['n_batches']\n",
    "sampler_size = train_sampler_params[\"size\"]\n",
    "sampler_replacement =  train_sampler_params['replacement'] \n",
    "mask_dir =  train_sampler_params['mask_dir'] \n",
    "sampler_set_seed =  train_sampler_params['set_seed'] \n",
    "sampler_roi_train = train_sampler_params['roi']\n",
    "static_dir = train_sampler_params['static_dir'] \n",
    "mask_threshold = 0.5 #train_sampler_params[\"mask_threshold\"]\n",
    "\n",
    "sampler_roi_test = test_sampler_params['roi']\n",
    "sampler_length_te = test_sampler_params['length'] \n",
    "#sampler_roi_val = val_sampler_params['roi']\n",
    "#sampler_length_val = val_sampler_params['length'] \n",
    "\n",
    "end_time = time.time()\n",
    "print(f'Took {end_time-start_time} seconds')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "start_time = time.time()\n",
    "# ==================================================================================================\n",
    "# Experiment settings\n",
    "# make a UUID based on the host ID and current time\n",
    "# will be saved to a .pkl file   \n",
    "\n",
    "\n",
    "Experiment_dict={}\n",
    "Experiment_dict['focus_time']=train_dataset_params['focus_time']\n",
    "Experiment_dict['focus_list']=train_dataset_params['focus_list']\n",
    "Experiment_dict['dataset_params']=train_dataset_params\n",
    "Model_hyperparameters_dict={}\n",
    "Universally_unique_identifier = uuid.uuid1()\n",
    "Experiment_dict[\"uuid\"]=Universally_unique_identifier.urn[9:]\n",
    "\n",
    "local_path = '/dccstor/cimf/drought_impact/architecture_experiments/tracking/' #dataset_params[\"local_path\"]\n",
    "#save_path = local_path+'Experiment_'+Universally_unique_identifier.urn[9:]+'/saves'\n",
    "save_path = local_path + 'test_'+Universally_unique_identifier.urn[9:]+'/saves'\n",
    "# Creating saving directory\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "end_time = time.time()\n",
    "print(f'Took {end_time-start_time} seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create samples\n",
    "Split temporally, all samples within same region\\\n",
    "If split spatially: define in dataset creation or in sampler ROI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took 0.12427043914794922 seconds\n",
      "Took 0.04250454902648926 seconds\n"
     ]
    }
   ],
   "source": [
    "# REMOVED ENV PATH and DEM FOR NOW \n",
    "\n",
    "start_time = time.time()\n",
    "train_ds = DroughtImpactDataset(s2_path=s2_path, era_path=era_path, env_path=env_path, dem_path=dem_path, focus_list=focus_list_train,\n",
    "                          focus_time=[focus_time_train], ts_delta=ts_delta, ts_len=ts_len, ratio=ratio, len_preds=len_preds, feature_set=feature_set, agg_funct_dict=agg_funct_dict, multiple_labels=multiple_labels,\n",
    "                               correct_ndvi=None)\n",
    "end_time = time.time()\n",
    "print(f'Took {end_time-start_time} seconds')\n",
    "\n",
    "\"\"\"\n",
    "start_time = time.time()\n",
    "val_ds = DroughtImpactDataset(s2_path=s2_path, era_path=era_path, env_path=env_path, dem_path=dem_path, focus_list=focus_list_val,\n",
    "                          focus_time=[focus_time_val], ts_delta=ts_delta, ts_len=ts_len, ratio=ratio, len_preds=len_preds, feature_set=feature_set, agg_funct_dict=agg_funct_dict, multiple_labels=multiple_labels)\n",
    "end_time = time.time()\n",
    "print(f'Took {end_time-start_time} seconds')\n",
    "\"\"\"\n",
    "start_time = time.time()\n",
    "test_ds = DroughtImpactDataset(s2_path=s2_path, era_path=era_path, env_path=env_path, dem_path=dem_path, focus_list=focus_list_test,\n",
    "                          focus_time=[focus_time_test], ts_delta=ts_delta, ts_len=ts_len, ratio=ratio, len_preds=len_preds, feature_set=feature_set, agg_funct_dict=agg_funct_dict, multiple_labels=multiple_labels)\n",
    "end_time = time.time()\n",
    "print(f'Took {end_time-start_time} seconds')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took 0.6423163414001465 seconds\n",
      "Took 0.3484668731689453 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "train_sampler = DroughtImpactSampler(train_ds, size=sampler_size, length=batch_size_tr*n_batch, replacement=sampler_replacement, \n",
    "                                     mask_dir=mask_dir, roi=sampler_roi_train, set_seed=sampler_set_seed,\n",
    "                                         mask_threshold=mask_threshold, static_dir=static_dir) \n",
    "end_time = time.time()\n",
    "print(f'Took {end_time-start_time} seconds')\n",
    "\"\"\"\n",
    "start_time = time.time()\n",
    "val_sampler = DroughtImpactSampler(val_ds, size=sampler_size, length=sampler_length_val, replacement=sampler_replacement, \n",
    "                                     mask_dir=mask_dir, set_seed=sampler_set_seed, static_dir=static_dir)\n",
    "end_time = time.time()\n",
    "print(f'Took {end_time-start_time} seconds')\n",
    "\"\"\"\n",
    "start_time = time.time()\n",
    "test_sampler = DroughtImpactSampler(test_ds, size=sampler_size, length=sampler_length_te, replacement=sampler_replacement, \n",
    "                                     mask_dir=mask_dir, set_seed=sampler_set_seed, static_dir=static_dir)\n",
    "end_time = time.time()\n",
    "print(f'Took {end_time-start_time} seconds')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_ds.all_loc_dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['46.907_7.137_47.407_7.637',\n",
       "  ['2020-05-04',\n",
       "   '2020-05-14',\n",
       "   '2020-05-24',\n",
       "   '2020-06-03',\n",
       "   '2020-06-13',\n",
       "   '2020-06-23',\n",
       "   '2020-07-03',\n",
       "   '2020-07-13',\n",
       "   '2020-07-23'],\n",
       "  [],\n",
       "  ['2020-08-02', '2020-08-12', '2020-08-22']],\n",
       " ['46.907_7.137_47.407_7.637',\n",
       "  ['2020-05-09',\n",
       "   '2020-05-19',\n",
       "   '2020-05-29',\n",
       "   '2020-06-08',\n",
       "   '2020-06-18',\n",
       "   '2020-06-28',\n",
       "   '2020-07-08',\n",
       "   '2020-07-18',\n",
       "   '2020-07-28'],\n",
       "  [],\n",
       "  ['2020-08-07', '2020-08-17', '2020-08-27']],\n",
       " ['46.907_7.137_47.407_7.637',\n",
       "  ['2020-05-14',\n",
       "   '2020-05-24',\n",
       "   '2020-06-03',\n",
       "   '2020-06-13',\n",
       "   '2020-06-23',\n",
       "   '2020-07-03',\n",
       "   '2020-07-13',\n",
       "   '2020-07-23',\n",
       "   '2020-08-02'],\n",
       "  [],\n",
       "  ['2020-08-12', '2020-08-22', '2020-09-01']],\n",
       " ['46.907_7.137_47.407_7.637',\n",
       "  ['2020-05-19',\n",
       "   '2020-05-29',\n",
       "   '2020-06-08',\n",
       "   '2020-06-18',\n",
       "   '2020-06-28',\n",
       "   '2020-07-08',\n",
       "   '2020-07-18',\n",
       "   '2020-07-28',\n",
       "   '2020-08-07'],\n",
       "  [],\n",
       "  ['2020-08-17', '2020-08-27', '2020-09-06']],\n",
       " ['46.907_7.137_47.407_7.637',\n",
       "  ['2020-05-24',\n",
       "   '2020-06-03',\n",
       "   '2020-06-13',\n",
       "   '2020-06-23',\n",
       "   '2020-07-03',\n",
       "   '2020-07-13',\n",
       "   '2020-07-23',\n",
       "   '2020-08-02',\n",
       "   '2020-08-12'],\n",
       "  [],\n",
       "  ['2020-08-22', '2020-09-01', '2020-09-11']],\n",
       " ['46.907_7.137_47.407_7.637',\n",
       "  ['2020-05-29',\n",
       "   '2020-06-08',\n",
       "   '2020-06-18',\n",
       "   '2020-06-28',\n",
       "   '2020-07-08',\n",
       "   '2020-07-18',\n",
       "   '2020-07-28',\n",
       "   '2020-08-07',\n",
       "   '2020-08-17'],\n",
       "  [],\n",
       "  ['2020-08-27', '2020-09-06', '2020-09-16']],\n",
       " ['46.907_7.137_47.407_7.637',\n",
       "  ['2020-06-03',\n",
       "   '2020-06-13',\n",
       "   '2020-06-23',\n",
       "   '2020-07-03',\n",
       "   '2020-07-13',\n",
       "   '2020-07-23',\n",
       "   '2020-08-02',\n",
       "   '2020-08-12',\n",
       "   '2020-08-22'],\n",
       "  [],\n",
       "  ['2020-09-01', '2020-09-11', '2020-09-21']],\n",
       " ['46.907_7.137_47.407_7.637',\n",
       "  ['2020-06-08',\n",
       "   '2020-06-18',\n",
       "   '2020-06-28',\n",
       "   '2020-07-08',\n",
       "   '2020-07-18',\n",
       "   '2020-07-28',\n",
       "   '2020-08-07',\n",
       "   '2020-08-17',\n",
       "   '2020-08-27'],\n",
       "  [],\n",
       "  ['2020-09-06', '2020-09-16', '2020-09-26']],\n",
       " ['46.907_7.137_47.407_7.637',\n",
       "  ['2020-06-13',\n",
       "   '2020-06-23',\n",
       "   '2020-07-03',\n",
       "   '2020-07-13',\n",
       "   '2020-07-23',\n",
       "   '2020-08-02',\n",
       "   '2020-08-12',\n",
       "   '2020-08-22',\n",
       "   '2020-09-01'],\n",
       "  [],\n",
       "  ['2020-09-11', '2020-09-21', '2020-10-01']]]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_ds.all_loc_dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open('all_loc_dates_val.pkl', 'wb') as f:\n",
    "    pickle.dump(val_ds.all_loc_dates, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open('all_loc_dates_test.pkl', 'wb') as f:\n",
    "    pickle.dump(test_ds.all_loc_dates, f)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "with open('all_loc_dates_2019p1.pkl', 'wb') as f:\n",
    "    pickle.dump(train_ds.all_loc_dates, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open('full_date_range.pkl', 'wb') as f:\n",
    "    pickle.dump(test_ds.full_date_range, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloader "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took 0.00040268898010253906 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "# Call the dataloaders\n",
    "train_dl = DataLoader(test_ds, sampler=test_sampler)  #, num_workers=2)\n",
    "#val_dl = DataLoader(val_ds, sampler=val_sampler) \n",
    "test_dl = DataLoader(test_ds, sampler=test_sampler) \n",
    "end_time = time.time()\n",
    "print(f'Took {end_time-start_time} seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Timing the data generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterating through dataloader: 38.15724492073059 sec\n"
     ]
    }
   ],
   "source": [
    "# OPTIMIZED \n",
    "start = time.time()\n",
    "test_samples = list(test_dl.__iter__())\n",
    "end = time.time()\n",
    "print(f'Iterating through dataloader: {end-start} sec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 8\n",
    "img = train_samples[idx][0]\n",
    "label = train_samples[idx][1]\n",
    "train_samples[idx][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "label[:,:,train_ds.feature_set['NDVI'],:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds.all_loc_dates[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img[:,:,train_ds.feature_set['CP'],:,:] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp_data = th.cat([img[:,:,train_ds.feature_set['CP'],:,:] ,label[:,:,train_ds.feature_set['CP'],:,:]], axis=1)\n",
    "b2_data = th.cat([img[:,:,train_ds.feature_set['B2'],:,:] ,label[:,:,train_ds.feature_set['B2'],:,:]], axis=1)\n",
    "b8_data = th.cat([img[:,:,train_ds.feature_set['B8'],:,:] ,label[:,:,train_ds.feature_set['B8'],:,:]], axis=1)\n",
    "ndvi_data = th.cat([img[:,:,train_ds.feature_set['NDVI'],:,:] ,label[:,:,train_ds.feature_set['NDVI'],:,:]], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b2_data <0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b8_data >0.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.interpolate import interp1d\n",
    "from scipy import signal\n",
    "\n",
    "def correct_noisy_ndvi(img, label, dataset, b2_thresh, b8_thresh, ts_len, len_preds):\n",
    "    \"\"\"\n",
    "    Impute NDVI through linear interpolation if blue band (B2) > b2_thresh and infrared band (B8) < b8_thresh.\n",
    "    Use both img and label for interpolation.\n",
    "    \n",
    "    Author: Selene\n",
    "    :param img: data tensor\n",
    "    :param label: label tensor\n",
    "    :param dataset: dataset\n",
    "    :param b2_thresh: threhsold for filtering with blue band (0.1 typically)\n",
    "    :param b8_thresh: threhsold for filtering with infrared band (0.15 typically)\n",
    "    :param ts_len: number of timestamps in data tensor\n",
    "    :param len_preds: number of timestamps in label tensor\n",
    "    \"\"\"\n",
    "    \n",
    "    print(img.shape)\n",
    "    img_tensor = img.clone().detach() \n",
    "    label_tensor = label.clone().detach() \n",
    "    \n",
    "    # Get band data for image and label\n",
    "    cp_data = th.cat([img_tensor[:,:,dataset.feature_set['CP'],:,:] ,label_tensor[:,:,dataset.feature_set['CP'],:,:]], axis=1)\n",
    "    b2_data = th.cat([img_tensor[:,:,dataset.feature_set['B2'],:,:] ,label_tensor[:,:,dataset.feature_set['B2'],:,:]], axis=1)\n",
    "    b8_data = th.cat([img_tensor[:,:,dataset.feature_set['B8'],:,:] ,label_tensor[:,:,dataset.feature_set['B8'],:,:]], axis=1)\n",
    "    ndvi_data = th.cat([img_tensor[:,:,dataset.feature_set['NDVI'],:,:] ,label_tensor[:,:,dataset.feature_set['NDVI'],:,:]], axis=1)\n",
    "\n",
    "    # Filter: find NDVI that needs to be replaced\n",
    "    to_rep = (cp_data>0) #((b2_data>b2_thresh) | (b8_data<b8_thresh))\n",
    "    to_rep = to_rep.squeeze(0).squeeze(1).squeeze(1) # make it 1D\n",
    "    \n",
    "    if to_rep.sum() < ts_len+len_preds-1: # There needs to be at least 2 points for inteprolation\n",
    "        print('cp')\n",
    "        # Linear interpolation \n",
    "        x = np.arange(ndvi_data.shape[1])[~to_rep] # get the x values that are valid\n",
    "        ndvi_vec = ndvi_data.squeeze(0).squeeze(1).squeeze(1) # make it 1D\n",
    "        f = interp1d(x, ndvi_vec[~to_rep], fill_value=\"extrapolate\")\n",
    "        interpolated = f(np.arange(ts_len+len_preds))\n",
    "        \n",
    "        # Make sure its between 0 and 1\n",
    "        interpolated[interpolated < 0] = 0\n",
    "        interpolated[interpolated > 1] = 1\n",
    "            \n",
    "        # Replace NDVI\n",
    "        img_tensor[:,:,dataset.feature_set['NDVI'],:,:] = th.from_numpy(interpolated[:ts_len]).unsqueeze(0).unsqueeze(2).unsqueeze(2)\n",
    "        label_tensor[:,:,dataset.feature_set['NDVI'],:,:] = th.from_numpy(interpolated[ts_len:]).unsqueeze(0).unsqueeze(2).unsqueeze(2)\n",
    "        \n",
    "        # Correct B2 and B8\n",
    "        img_tensor[:,to_rep[:ts_len],dataset.feature_set['B2'],:,:] = b2_thresh\n",
    "        label_tensor[:,to_rep[ts_len:],dataset.feature_set['B2'],:,:] = b2_thresh\n",
    "        img_tensor[:,to_rep[:ts_len],dataset.feature_set['B8'],:,:] = b8_thresh\n",
    "        label_tensor[:,to_rep[ts_len:],dataset.feature_set['B8'],:,:] = b8_thresh\n",
    "        # Set CP to 0 for all values\n",
    "        img_tensor[:,:,dataset.feature_set['CP'],:,:] = 0\n",
    "        label_tensor[:,:,dataset.feature_set['CP'],:,:] = 0\n",
    "    \n",
    "    # If everything gets dropped\n",
    "    else:\n",
    "        to_rep = ((b2_data>b2_thresh) | (b8_data<b8_thresh))\n",
    "        to_rep = to_rep.squeeze(0).squeeze(1).squeeze(1) # make it 1D\n",
    "        print('band', to_rep.sum())\n",
    "        # Linear interpolation \n",
    "        x = np.arange(ndvi_data.shape[1])[~to_rep] # get the x values that are valid\n",
    "        ndvi_vec = ndvi_data.squeeze(0).squeeze(1).squeeze(1) # make it 1D\n",
    "        f = interp1d(x, ndvi_vec[~to_rep], fill_value=\"extrapolate\")\n",
    "        interpolated = f(np.arange(ts_len+len_preds))\n",
    "        \n",
    "        # Make sure its between 0 and 1\n",
    "        interpolated[interpolated < 0] = 0\n",
    "        interpolated[interpolated > 1] = 1\n",
    "            \n",
    "        # Replace NDVI\n",
    "        img_tensor[:,:,dataset.feature_set['NDVI'],:,:] = th.from_numpy(interpolated[:ts_len]).unsqueeze(0).unsqueeze(2).unsqueeze(2)\n",
    "        label_tensor[:,:,dataset.feature_set['NDVI'],:,:] = th.from_numpy(interpolated[ts_len:]).unsqueeze(0).unsqueeze(2).unsqueeze(2)\n",
    "\n",
    "        # Set CP to 0 for all values\n",
    "        img_tensor[:,:,dataset.feature_set['CP'],:,:] = 0\n",
    "        label_tensor[:,:,dataset.feature_set['CP'],:,:] = 0\n",
    "    \n",
    "    \n",
    "    return img_tensor, label_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "img2, lab2 = correct_noisy_ndvi(img, label, train_ds, 0.10, 0.15, 15, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot before and after correction\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Get band data for image and label\n",
    "cp_data = th.cat([img[:,:,train_ds.feature_set['CP'],:,:] ,label[:,:,train_ds.feature_set['CP'],:,:]], axis=1).squeeze(0).squeeze(1).squeeze(1)\n",
    "b2_data = th.cat([img[:,:,train_ds.feature_set['B2'],:,:] ,label[:,:,train_ds.feature_set['B2'],:,:]], axis=1).squeeze(0).squeeze(1).squeeze(1)\n",
    "b8_data = th.cat([img[:,:,train_ds.feature_set['B8'],:,:] ,label[:,:,train_ds.feature_set['B8'],:,:]], axis=1).squeeze(0).squeeze(1).squeeze(1)\n",
    "ndvi_data = th.cat([img[:,:,train_ds.feature_set['NDVI'],:,:] ,label[:,:,train_ds.feature_set['NDVI'],:,:]], axis=1).squeeze(0).squeeze(1).squeeze(1)\n",
    "\n",
    "# Get band data for image and label\n",
    "cp_data2 = th.cat([img2[:,:,train_ds.feature_set['CP'],:,:] ,lab2[:,:,train_ds.feature_set['CP'],:,:]], axis=1).squeeze(0).squeeze(1).squeeze(1)\n",
    "b2_data2 = th.cat([img2[:,:,train_ds.feature_set['B2'],:,:] ,lab2[:,:,train_ds.feature_set['B2'],:,:]], axis=1).squeeze(0).squeeze(1).squeeze(1)\n",
    "b8_data2 = th.cat([img2[:,:,train_ds.feature_set['B8'],:,:] ,lab2[:,:,train_ds.feature_set['B8'],:,:]], axis=1).squeeze(0).squeeze(1).squeeze(1)\n",
    "ndvi_data2 = th.cat([img2[:,:,train_ds.feature_set['NDVI'],:,:] ,lab2[:,:,train_ds.feature_set['NDVI'],:,:]], axis=1).squeeze(0).squeeze(1).squeeze(1)\n",
    "   \n",
    "\n",
    "fig, axs = plt.subplots(1, 1, figsize=(10, 3))\n",
    "plt.suptitle('Raw data')\n",
    "sns.lineplot(ax=axs, x=np.arange(18), y=ndvi_data, label='NDVI')\n",
    "sns.lineplot(ax=axs, x=np.arange(18), y=cp_data/100, label='CP')\n",
    "sns.lineplot(ax=axs, x=np.arange(18), y=b2_data, label='B2')\n",
    "sns.lineplot(ax=axs, x=np.arange(18), y=b8_data, label='B8')\n",
    "sns.despine(top=True, right=True)\n",
    "plt.tight_layout()\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(1, 1, figsize=(10, 3))\n",
    "plt.suptitle('Correctd data')\n",
    "sns.lineplot(ax=axs, x=np.arange(18), y=ndvi_data2, label='NDVI')\n",
    "sns.lineplot(ax=axs, x=np.arange(18), y=cp_data2/100, label='CP')\n",
    "sns.lineplot(ax=axs, x=np.arange(18), y=b2_data2, label='B2')\n",
    "sns.lineplot(ax=axs, x=np.arange(18), y=b8_data2, label='B8')\n",
    "sns.despine(top=True, right=True)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndvi_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndvi_data2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Timing the data loading from memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "x,y = load_batch(batch_size = 10, batch_nbr = 0, sample_type = 'pixel_data', split='train', exp='arch')\n",
    "end = time.time()\n",
    "print(f'Loading a batch: {end-start} sec')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import cProfile\n",
    "\n",
    "profiler = cProfile.Profile()\n",
    "profiler.enable()\n",
    "\n",
    "for _ in train_sampler:\n",
    "    pass\n",
    "\n",
    "profiler.disable()\n",
    "profiler.print_stats()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "tags": []
   },
   "source": [
    "start = time.time()\n",
    "for idx, c in islice(enumerate(train_dl), 0, 5): \n",
    "    img, labl = c[0], c[1]\n",
    "end = time.time()\n",
    "print(f'Iterating through a batch: {end-start} sec')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import cProfile\n",
    "pr = cProfile.Profile()\n",
    "pr.enable()\n",
    "\n",
    "train_samples = list(train_dl.__iter__())\n",
    "\n",
    "pr.disable()\n",
    "pr.print_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stats on train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# ==================================================================================================\n",
    "# Do statistics on the training set to normalize the entire dataset\n",
    "\n",
    "# Temporal bands\n",
    "bands = []\n",
    "n_temp = train_ds.bands_s2 + train_ds.bands_era\n",
    "for i in list(train_ds.feature_set.keys())[:n_temp]:\n",
    "    bands.append(train_ds.feature_set[i])\n",
    "tmp_bands_vals, tmp_band_means_or_mins, tmp_band_stds_or_maxs = dataset_stats(train_dl, bands=bands, temporal=True, norm_method=sim_params[\"norm_method\"])\n",
    "\n",
    "# Static bands\n",
    "bands = []\n",
    "for i in list(train_ds.feature_set.keys())[n_temp:]:\n",
    "    bands.append(train_ds.feature_set[i])\n",
    "stat_bands_vals, stat_band_means_or_mins, stat_band_stds_or_maxs = dataset_stats(train_dl, bands=bands, temporal=False, norm_method=sim_params[\"norm_method\"])\n",
    "\n",
    "# First temporal, then static\n",
    "all_band_vals = list(tmp_bands_vals) + list(stat_bands_vals)\n",
    "all_band_means_or_mins = list(tmp_band_means_or_mins) + list(stat_band_means_or_mins)\n",
    "all_band_stds_or_maxs = list(tmp_band_stds_or_maxs) + list(stat_band_stds_or_maxs)\n",
    "\n",
    "Model_hyperparameters_dict['mean_or_min_intensity_training_set']=all_band_means_or_mins\n",
    "Model_hyperparameters_dict['std_or_max_intensity_training_set']=all_band_stds_or_maxs\n",
    "\n",
    "end_time = time.time()\n",
    "print(f'Took {end_time-start_time} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ==================================================================================================\n",
    "# Perform normalization and prepare dataloaders\n",
    "start_time = time.time()\n",
    "train_ds_norm = DroughtImpactDataset(s2_path=s2_path, era_path=era_path, dem_path=dem_path, env_path=env_path, focus_list=focus_list_train,\n",
    "                          focus_time=[focus_time_train], ts_delta=ts_delta, ts_len=ts_len, ratio=ratio, len_preds=len_preds, feature_set=feature_set, agg_funct_dict=agg_funct_dict, norm_stats=[all_band_means_or_mins, all_band_stds_or_maxs], norm_method=sim_params[\"norm_method\"], multiple_labels=multiple_labels)\n",
    "\n",
    "val_ds_norm = DroughtImpactDataset(s2_path=s2_path, era_path=era_path, dem_path=dem_path, env_path=env_path,  focus_list=focus_list_val,\n",
    "                          focus_time=[focus_time_val], ts_delta=ts_delta, ts_len=ts_len, ratio=ratio, len_preds=len_preds, feature_set=feature_set, agg_funct_dict=agg_funct_dict, norm_stats=[all_band_means_or_mins, all_band_stds_or_maxs], norm_method=sim_params[\"norm_method\"], multiple_labels=multiple_labels)\n",
    "\n",
    "test_ds_norm = DroughtImpactDataset(s2_path=s2_path, era_path=era_path, dem_path=dem_path, env_path=env_path,  focus_list=focus_list_test,\n",
    "                          focus_time=[focus_time_test1,focus_time_test2], ts_delta=ts_delta, ts_len=ts_len, ratio=ratio, len_preds=len_preds, feature_set=feature_set, agg_funct_dict=agg_funct_dict, norm_stats=[all_band_means_or_mins, all_band_stds_or_maxs], norm_method=sim_params[\"norm_method\"], multiple_labels=multiple_labels)\n",
    "\n",
    "train_sampler_norm = DroughtImpactSampler(train_ds_norm, size=sampler_size, length=batch_size_tr*n_batch, replacement=sampler_replacement, \n",
    "                                     mask_dir=mask_dir, set_seed=sampler_set_seed, static_dir=static_dir)\n",
    "\n",
    "val_sampler_norm = DroughtImpactSampler(val_ds_norm, size=sampler_size, length=batch_size_tr*n_batch, replacement=sampler_replacement, \n",
    "                                     mask_dir=mask_dir, set_seed=sampler_set_seed, static_dir=static_dir)\n",
    "\n",
    "test_sampler_norm = DroughtImpactSampler(test_ds_norm, size=sampler_size, length=sampler_length_te, replacement=sampler_replacement, \n",
    "                                     mask_dir=mask_dir, set_seed=sampler_set_seed, static_dir=static_dir)\n",
    "\n",
    "\n",
    "# Call the dataloaders\n",
    "train_dl_norm = DataLoader(train_ds_norm, sampler=train_sampler_norm) \n",
    "val_dl_norm = DataLoader(val_ds_norm, sampler=val_sampler_norm) \n",
    "test_dl_norm = DataLoader(test_ds_norm, sampler=test_sampler_norm) \n",
    "end_time = time.time()\n",
    "print(f'Took {end_time-start_time} seconds')\n",
    "\n",
    "start_time = time.time()\n",
    "# Get info on each set \n",
    "## Maybe not efficient to go through samples...\n",
    "train_samples = list(train_dl_norm.__iter__())\n",
    "called_train_samples = [x[2] for x in train_samples]\n",
    "val_samples = list(val_dl_norm.__iter__())\n",
    "called_val_samples = [x[2] for x in val_samples]\n",
    "test_samples = list(test_dl_norm.__iter__())\n",
    "called_test_samples = [x[2] for x in test_samples]\n",
    "\n",
    "# Save info to experiment\n",
    "Experiment_dict['train_samples']=called_train_samples\n",
    "Experiment_dict['val_samples']=called_val_samples\n",
    "Experiment_dict['test_samples']=called_test_samples\n",
    "\n",
    "\n",
    "end_time = time.time()\n",
    "print(f'Took {end_time-start_time} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_samples[4][0].isnan().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Get existing experiment\n",
    "method = 'rec'\n",
    "dt_string = 'potato_0.0001_2_10'\n",
    "experiment = mlflow.get_experiment_by_name(dt_string)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Get existing run and its ID to use when tracking\n",
    "client = mlflow.tracking.MlflowClient() # Create a MlflowClient object\n",
    "runs = client.search_runs(experiment.experiment_id)\n",
    "run_id = [r.info.run_id for r in runs if r.info.run_name==f'train_{method}'][0]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Log metric to existing run and experiment\n",
    "client.log_metric(run_id, \"test\", 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# Create folder where checkpoints for model will be saved\n",
    "method = sim_params[\"method\"] # direct vs oneshot\n",
    "\n",
    "checkpoint_folder = f'checkpoints/{method}_{sim_params[\"learning_rate\"]}_{ model_params[\"num_layers\"]}_{model_params[\"hidden_dim\"]}/'\n",
    "#checkpoint_folder = f'checkpoints/{dt_string}/'\n",
    "if not os.path.exists(checkpoint_folder):\n",
    "    os.mkdir(checkpoint_folder)\n",
    "    \n",
    "end_time = time.time()\n",
    "print(f'Took {end_time-start_time} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_string = 'debug'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_file_prefix = 'checkpoint_'+dt_string.split(' ')[0].replace('/', '_')+'_e'\n",
    "checkpoints = [file for file in os.listdir(checkpoint_folder) if file.startswith(checkpoint_file_prefix) and 'best' not in file]\n",
    "sorted_checkpoints = sorted(checkpoints, key=get_ckpt_epoch_batch)\n",
    "# Get last checkpoint (latest epoch)\n",
    "checkpoint_file = sorted_checkpoints[-1] if len(sorted_checkpoints)!=0 else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "checkpoint_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "if checkpoint_file is not None:\n",
    "    checkpoint = th.load(checkpoint_folder+checkpoint_file)\n",
    "    start_epoch = checkpoint['epoch']\n",
    "    start_batch = checkpoint['batch']\n",
    "    epoch_loss = checkpoint['epoch_loss']\n",
    "    optimizer = checkpoint['optimizer']\n",
    "    dt_string = checkpoint['experiment_name']\n",
    "    mlflow_run_id = checkpoint['mlflow_run_id']\n",
    "    \n",
    "    hidden_dim = model_params[\"hidden_dim\"]\n",
    "    num_layers = model_params[\"num_layers\"]\n",
    "    output_dim = model_params[\"output_dim\"]\n",
    "    \n",
    "    if method == 'dir': #direct\n",
    "        model = LSTM_oneshot(input_dim=len(train_ds.feature_set)-len(remove_bands), hidden_dim=hidden_dim, num_layers=num_layers, output_dim=output_dim)\n",
    "    if method == 'rec': #recursive\n",
    "        model = LSTM_recursive(input_dim=len(train_ds.feature_set)-len(remove_bands), hidden_dim=hidden_dim, num_layers=num_layers, num_steps=sim_params[\"num_steps\"])\n",
    "    criterion = select_loss_function(sim_params['loss_function'])\n",
    "    \n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    print(\"=> loaded checkpoint '{}' (trained for {} epochs)\".format(checkpoint_file, checkpoint['epoch']+1))\n",
    "    \n",
    "    # Get existing MLflow experiment\n",
    "    experiment = mlflow.get_experiment_by_name(dt_string)\n",
    "    \n",
    "    # Get run that was just created and its ID to use when tracking\n",
    "    client = mlflow.tracking.MlflowClient() # Create a MlflowClient object\n",
    "    runs = client.search_runs(experiment.experiment_id)\n",
    "    mlflow_run_id = [r.info.run_id for r in runs if r.info.run_name==f'train_{method}'][0]\n",
    "    mlflow_run_id_val = [r.info.run_id for r in runs if r.info.run_name==f'val_{method}'][0]\n",
    "    \n",
    "\n",
    "if checkpoint_file is None:\n",
    "    start_epoch = 0\n",
    "    start_batch = 0\n",
    "    epoch_loss = 0\n",
    "    hidden_dim = model_params[\"hidden_dim\"]\n",
    "    num_layers = model_params[\"num_layers\"]\n",
    "    output_dim = model_params[\"output_dim\"]\n",
    "    lr = sim_params[\"learning_rate\"] # learning rate\n",
    "    \n",
    "    if method == 'dir': #direct\n",
    "        model = LSTM_oneshot(input_dim=len(train_ds.feature_set)-len(remove_bands), hidden_dim=hidden_dim, num_layers=num_layers, output_dim=output_dim)\n",
    "    if method == 'rec': #recursive\n",
    "        model = LSTM_recursive(input_dim=len(train_ds.feature_set)-len(remove_bands), hidden_dim=hidden_dim, num_layers=num_layers, num_steps=sim_params[\"num_steps\"])\n",
    "    criterion = select_loss_function(sim_params['loss_function'])\n",
    "    optimizer = select_optimizer(sim_params[\"optimizer\"], model.parameters(), sim_params[\"learning_rate\"], sim_params[\"momentum\"])\n",
    "\n",
    "    #summary(model, (len(train_ds.feature_set)-len(remove_bands), 1, 1))\n",
    "    \n",
    "    # Create new MLflow experiment\n",
    "    now = datetime.now()\n",
    "    #dt_string = now.strftime(\"%d/%m/%Y\")+f'_{sim_params[\"learning_rate\"]}_{model_params[\"num_layers\"]}_{model_params[\"hidden_dim\"]}'\n",
    "    dt_string = 'debug'\n",
    "    mlflow.create_experiment(name=dt_string) \n",
    "    experiment = mlflow.get_experiment_by_name(dt_string)\n",
    "    \n",
    "    with mlflow.start_run(experiment_id = experiment.experiment_id, run_name=f'train_{method}'):\n",
    "        mlflow.log_param(\"n_samples training\", len(train_dl))\n",
    "        mlflow.log_param(\"batch_size training\", batch_size_tr)\n",
    "    \n",
    "    \n",
    "    with mlflow.start_run(experiment_id = experiment.experiment_id, run_name=f'val_{method}'):\n",
    "        mlflow.log_param(f\"n_samples val\", len(val_dl))\n",
    "        \n",
    "    # Get run that was just created and its ID to use when tracking\n",
    "    client = mlflow.tracking.MlflowClient() # Create a MlflowClient object\n",
    "    runs = client.search_runs(experiment.experiment_id)\n",
    "    mlflow_run_id = [r.info.run_id for r in runs if r.info.run_name==f'train_{method}'][0]\n",
    "    mlflow_run_id_val = [r.info.run_id for r in runs if r.info.run_name==f'val_{method}'][0]\n",
    "    \n",
    "end_time = time.time()\n",
    "print(f'Took {end_time-start_time} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "total_tr_loss = 0\n",
    "best_loss = np.inf\n",
    "model.train()\n",
    "\n",
    "for ix_epoch in tqdm(range(1)): #sim_params[\"num_epochs\"])\n",
    "    if ix_epoch<start_epoch:\n",
    "        continue\n",
    "\n",
    "    print(f\"Epoch {ix_epoch}\\n---------\")\n",
    "\n",
    "    # Train\n",
    "    epoch_loss = train_model(method=method, data_loader=train_dl_norm, model=model, epoch=ix_epoch, loss_function=criterion, optimizer=optimizer, \n",
    "                             batch_size=batch_size_tr, n_batch=n_batch,\n",
    "                             n_timesteps_in=ts_len, n_timesteps_out=len_preds, n_feats_in=len(train_ds_norm.feature_set)-len(remove_bands), n_feats_out=output_dim, \n",
    "                             remove_band=train_dataset_params[\"remove_bands\"], feature_set=train_ds_norm.feature_set, \n",
    "                             experiment=experiment, checkpoint_folder=checkpoint_folder, dt_string=dt_string, start_batch=start_batch, client=client, run_id=mlflow_run_id, epoch_loss=epoch_loss)\n",
    "\n",
    "    total_tr_loss += epoch_loss\n",
    "    \n",
    "    end_time = time.time()\n",
    "    print(f'Took {end_time-start_time} seconds')    \n",
    "\n",
    "    # Validate\n",
    "    total_val_loss, avg_val_loss = test_model(method=method, data_loader=val_dl_norm, model=model, loss_function=criterion, \n",
    "                                              n_timesteps_in=ts_len, n_timesteps_out=len_preds, n_feats_in=len(train_ds_norm.feature_set)-len(remove_bands), \n",
    "                                              n_feats_out=output_dim, remove_band=train_dataset_params[\"remove_bands\"], feature_set=train_ds_norm.feature_set, \n",
    "                                              experiment=experiment, split='val', client=client, run_id=mlflow_run_id_val, checkpoint_folder=checkpoint_folder)\n",
    "\n",
    "\n",
    "\n",
    "    best_loss = compare_model_for_checkpoint(total_val_loss, best_loss, model, ix_epoch, checkpoint_folder+'checkpoint_'+dt_string.split(' ')[0].replace('/', '_')+f'_e{ix_epoch}_b{n_batch}_best.pth.tar') \n",
    "\n",
    "with mlflow.start_run(experiment_id = experiment.experiment_id, run_name='trained'):\n",
    "    mlflow.sklearn.log_model(model, \"model\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " ########################################################################\n",
    "# TEST MODEL\n",
    "with mlflow.start_run(experiment_id = experiment.experiment_id, run_name=f'test_{method}'):\n",
    "        mlflow.log_param(f\"n_samples val\", len(val_dl_norm))\n",
    "        \n",
    "# Get run that was just created and its ID to use when tracking\n",
    "client = mlflow.tracking.MlflowClient() # Create a MlflowClient object\n",
    "runs = client.search_runs(experiment.experiment_id)\n",
    "mlflow_run_id_test = [r.info.run_id for r in runs if r.info.run_name==f'test_{method}'][0]\n",
    "\n",
    "    \n",
    "total_test_loss, avg_test_loss = test_model(method=method, data_loader=test_dl_norm, model=model, loss_function=criterion, \n",
    "                                            n_timesteps_in=ts_len, n_timesteps_out=len_preds, n_feats_in=len(train_ds.feature_set)-len(remove_bands), n_feats_out=output_dim, \n",
    "                                            remove_band=train_dataset_params[\"remove_bands\"], feature_set=test_ds_norm.feature_set, \n",
    "                                            experiment=experiment, split='test', client=client, run_id=mlflow_run_id_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.end_run(mlflow_run_id)\n",
    "mlflow.end_run(mlflow_run_id_val)\n",
    "mlflow.end_run(mlflow_run_id_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Model_hyperparameters_dict['loss_function']=sim_params['loss_function']\n",
    "Model_hyperparameters_dict['model_architecture']=model\n",
    "Model_hyperparameters_dict['learning_rate']=sim_params['learning_rate']\n",
    "Model_hyperparameters_dict['optimizer']=sim_params['optimizer']\n",
    "Model_hyperparameters_dict['num_epochs']=sim_params['num_epochs']\n",
    "Model_hyperparameters_dict['batch_size_train']=train_sampler_params['batch_size']\n",
    "\n",
    "# Save Statistics\n",
    "Experiment_dict['Model_hyperparameters']=Model_hyperparameters_dict\n",
    "pd.to_pickle(Experiment_dict,save_path+'/Experiment_dict_'+Universally_unique_identifier.urn[9:]+'.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": [
     "\"\"\"\n",
     "ZRL PHACT GROUP\n",
     "\n",
     "Pytorch Segmentation template\n",
     "\n",
     "Authors:    Michal Muszynski (MMU@zurich.ibm.com)\n",
     "            Jonas Weiss (jwe@zurich.ibm.com)\n",
     "            Thomas Brunschwiler (tbr@zurich.ibm.com)\n",
     "            Paolo Fraccaro (Paolo.Fraccaro@ibm.com)\n",
     "\n",
     "Created:    8-August-2022\n",
     "\"\"\""
    ]
   }
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
