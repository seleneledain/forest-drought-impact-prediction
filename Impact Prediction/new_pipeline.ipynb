{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/u/seleneledain/.conda/envs/drought2/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x154934243a90>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ==================================================================================================\n",
    "# import packages\n",
    "from pathlib import Path\n",
    "import torch as th\n",
    "from torch.utils.data import DataLoader\n",
    "import time\n",
    "from tqdm import tqdm # Instantly make your loops show a smart progress meter\n",
    "import os\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchsummary import summary\n",
    "from PIL import Image\n",
    "import uuid\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Import other scripts here \n",
    "from drought_impact_dataset import *\n",
    "from drought_impact_sampler import *\n",
    "# Import a config file for training \n",
    "from utils.train_config_rec import * #train_dataset_params, train_sampler_params, test_dataset_params, test_sampler_params, sim_params, model_params\n",
    "from torch.utils.data import DataLoader\n",
    "from utils.utils_pixel import *\n",
    "from model import *\n",
    "\n",
    "# MLFlow\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "import logging\n",
    "logging.basicConfig(level=logging.WARN)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "th.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================================================================\n",
    "# Get setup details from configuration file\n",
    "\n",
    "feature_set = train_dataset_params['feature_set']\n",
    "remove_bands = train_dataset_params['remove_bands']\n",
    "multiple_labels = train_dataset_params['multiple_labels']\n",
    "batch_size_tr =  train_sampler_params['batch_size'] \n",
    "n_batch = 2 #sim_params['n_batches']\n",
    "ts_len =  train_dataset_params['ts_len']\n",
    "len_preds =  1 #train_dataset_params['len_preds']\n",
    "\n",
    "n_train = batch_size_tr*n_batch\n",
    "batch_size_val = 10 #sim_params['batch_size_val'] \n",
    "n_batch_val = 2 #sim_params['n_batches_val']\n",
    "batch_size_te = 10 # sim_params['batch_size_te'] \n",
    "n_batch_te = 2 #sim_params['n_batches_te']\n",
    "exp = sim_params[\"exp\"]\n",
    "sample_type = sim_params[\"sample_type\"]\n",
    "method = sim_params[\"method\"] # direct vs oneshot\n",
    "exp = sim_params[\"exp\"]\n",
    "exp_val = sim_params[\"exp_val\"]\n",
    "exp_te = sim_params[\"exp_test\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create folder where checkpoints for model will be saved\n",
    "\n",
    "checkpoint_folder = f'checkpoints/{method}_{sim_params[\"learning_rate\"]}_{model_params[\"num_layers\"]}_{model_params[\"hidden_dim\"]}/'\n",
    "#checkpoint_folder = f'checkpoints/{dt_string}/'\n",
    "if not os.path.exists(checkpoint_folder):\n",
    "    os.mkdir(checkpoint_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "now = datetime.now()\n",
    "dt_string = now.strftime(\"%d/%m/%Y\")+f'_{method}_{sim_params[\"learning_rate\"]}_{model_params[\"num_layers\"]}_{model_params[\"hidden_dim\"]}'\n",
    "\n",
    "checkpoint_file_prefix = 'checkpoint_'+dt_string.split(' ')[0].replace('/', '_')+'_e'\n",
    "checkpoints = [file for file in os.listdir(checkpoint_folder) if file.startswith(checkpoint_file_prefix) and 'best' not in file]\n",
    "sorted_checkpoints = sorted(checkpoints, key=get_ckpt_epoch_batch)\n",
    "# Get last checkpoint (latest epoch)\n",
    "checkpoint_file = sorted_checkpoints[-1] if len(sorted_checkpoints)!=0 else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "checkpoint_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if checkpoint_file is not None:\n",
    "    checkpoint = th.load(checkpoint_folder+checkpoint_file)\n",
    "    start_epoch = checkpoint['epoch']\n",
    "    start_batch = checkpoint['batch']\n",
    "    epoch_loss = checkpoint['epoch_loss']\n",
    "    optimizer = checkpoint['optimizer']\n",
    "    dt_string = checkpoint['experiment_name']\n",
    "    mlflow_run_id = checkpoint['mlflow_run_id']\n",
    "    \n",
    "    hidden_dim = model_params[\"hidden_dim\"]\n",
    "    num_layers = model_params[\"num_layers\"]\n",
    "    output_dim = model_params[\"output_dim\"]\n",
    "    \n",
    "    if method == 'dir': #direct\n",
    "        model = LSTM_oneshot(input_dim=len(feature_set)-len(remove_bands), hidden_dim=hidden_dim, num_layers=num_layers, output_dim=output_dim)\n",
    "    if method == 'rec': #recursive\n",
    "        model = LSTM_recursive(input_dim=len(feature_set)-len(remove_bands), hidden_dim=hidden_dim, num_layers=num_layers, num_steps=sim_params[\"num_steps\"])\n",
    "    criterion = select_loss_function(sim_params['loss_function'])\n",
    "    \n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    print(\"=> loaded checkpoint '{}' (trained for {} epochs)\".format(checkpoint_file, checkpoint['epoch']+1))\n",
    "    \n",
    "    # Get existing MLflow experiment\n",
    "    experiment = mlflow.get_experiment_by_name(dt_string)\n",
    "    \n",
    "    # Get run that was just created and its ID to use when tracking\n",
    "    client = mlflow.tracking.MlflowClient() # Create a MlflowClient object\n",
    "    runs = client.search_runs(experiment.experiment_id)\n",
    "    mlflow_run_id = [r.info.run_id for r in runs if r.info.run_name==f'train_{method}'][0]\n",
    "    mlflow_run_id_val = [r.info.run_id for r in runs if r.info.run_name==f'val_{method}'][0]\n",
    "    \n",
    "\n",
    "if checkpoint_file is None:\n",
    "    start_epoch = 0\n",
    "    start_batch = 0\n",
    "    epoch_loss = 0\n",
    "    hidden_dim = model_params[\"hidden_dim\"]\n",
    "    num_layers = model_params[\"num_layers\"]\n",
    "    output_dim = model_params[\"output_dim\"]\n",
    "    lr = sim_params[\"learning_rate\"] # learning rate\n",
    "    \n",
    "    if method == 'dir': #direct\n",
    "        model = LSTM_oneshot(input_dim=len(feature_set)-len(remove_bands), hidden_dim=hidden_dim, num_layers=num_layers, output_dim=output_dim)\n",
    "    if method == 'rec': #recursive\n",
    "        model = LSTM_recursive(input_dim=len(feature_set)-len(remove_bands), hidden_dim=hidden_dim, num_layers=num_layers, num_steps=sim_params[\"num_steps\"])\n",
    "    criterion = select_loss_function(sim_params['loss_function'])\n",
    "    optimizer = select_optimizer(sim_params[\"optimizer\"], model.parameters(), sim_params[\"learning_rate\"], sim_params[\"momentum\"])\n",
    "\n",
    "    #summary(model, (len(train_ds.feature_set)-len(remove_bands), 1, 1))\n",
    "    \n",
    "    # Create new MLflow experiment\n",
    "    now = datetime.now()\n",
    "    dt_string = now.strftime(\"%d/%m/%Y\")+f'_{sim_params[\"learning_rate\"]}_{model_params[\"num_layers\"]}_{model_params[\"hidden_dim\"]}'\n",
    "    #dt_string = 'debug'\n",
    "    mlflow.create_experiment(name=dt_string) \n",
    "    experiment = mlflow.get_experiment_by_name(dt_string)\n",
    "    \n",
    "    with mlflow.start_run(experiment_id = experiment.experiment_id, run_name=f'train_{method}'):\n",
    "        mlflow.log_param(\"n_samples training\", n_train)\n",
    "        mlflow.log_param(\"batch_size training\", batch_size_tr)\n",
    "    \n",
    "    \n",
    "    with mlflow.start_run(experiment_id = experiment.experiment_id, run_name=f'val_{method}'):\n",
    "        mlflow.log_param(f\"n_samples val\", batch_size_val*n_batch_val)\n",
    "        \n",
    "    # Get run that was just created and its ID to use when tracking\n",
    "    client = mlflow.tracking.MlflowClient() # Create a MlflowClient object\n",
    "    runs = client.search_runs(experiment.experiment_id)\n",
    "    mlflow_run_id = [r.info.run_id for r in runs if r.info.run_name==f'train_{method}'][0]\n",
    "    mlflow_run_id_val = [r.info.run_id for r in runs if r.info.run_name==f'val_{method}'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                            | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "---------\n",
      "Batch nbr 0. Average batch loss: 0.007998641580343246\n",
      "Time for a batch: 0.19682884216308594 sec\n",
      "Batch nbr 1. Average batch loss: 0.007397109270095825\n",
      "Time for a batch: 0.15918207168579102 sec\n",
      "val-ing model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████| 1/1 [00:00<00:00,  1.93it/s]\n",
      "2023/01/30 11:55:49 WARNING mlflow.sklearn: Model was missing function: predict. Not logging python_function flavor!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch nbr 0. Average batch loss: 0.04717507362365723\n",
      "Batch nbr 1. Average batch loss: 0.050290465354919434\n",
      "=> Saving a new best\n"
     ]
    }
   ],
   "source": [
    "total_tr_loss = 0\n",
    "best_loss = np.inf\n",
    "model.train()\n",
    "\n",
    "for ix_epoch in tqdm(range(1)): #sim_params[\"num_epochs\"])\n",
    "    if ix_epoch<start_epoch:\n",
    "        continue\n",
    "\n",
    "    print(f\"Epoch {ix_epoch}\\n---------\")\n",
    "\n",
    "    # Train\n",
    "    epoch_loss = train_model(method=method, model=model, epoch=ix_epoch, loss_function=criterion, optimizer=optimizer, \n",
    "                             batch_size=batch_size_tr, n_batch=n_batch,\n",
    "                             n_timesteps_in=ts_len, n_timesteps_out=len_preds, n_feats_in=len(feature_set)-len(remove_bands), n_feats_out=output_dim, \n",
    "                             remove_band=remove_bands, feature_set=feature_set, \n",
    "                             experiment=experiment, checkpoint_folder=checkpoint_folder, dt_string=dt_string, start_batch=start_batch, client=client, run_id=mlflow_run_id, epoch_loss=epoch_loss,\n",
    "                             sample_type=sample_type, exp=exp, cp_idx=(0,1))\n",
    "\n",
    "    total_tr_loss += epoch_loss\n",
    "    \n",
    "\n",
    "    # Validate\n",
    "        \n",
    "    total_val_loss = test_model(method=method, model=model, epoch=ix_epoch, loss_function=criterion, \n",
    "                             batch_size=batch_size_val, n_batch=n_batch_val,\n",
    "                             n_timesteps_in=ts_len, n_timesteps_out=len_preds, n_feats_in=len(feature_set)-len(remove_bands), n_feats_out=output_dim, \n",
    "                             remove_band=remove_bands, feature_set=feature_set, \n",
    "                             experiment=experiment, split='val', start_batch=start_batch, client=client, run_id=mlflow_run_id, checkpoint_folder=checkpoint_folder,\n",
    "                             sample_type=sample_type, exp=exp_val, cp_idx=(0,1))\n",
    "\n",
    "    best_loss = compare_model_for_checkpoint(total_val_loss, best_loss, model, ix_epoch, checkpoint_folder+'checkpoint_'+dt_string.split(' ')[0].replace('/', '_')+f'_e{ix_epoch}_b{n_batch}_best.pth.tar') \n",
    "\n",
    "with mlflow.start_run(experiment_id = experiment.experiment_id, run_name='trained'):\n",
    "    mlflow.sklearn.log_model(model, \"model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    " ########################################################################\n",
    "# TEST MODEL\n",
    "with mlflow.start_run(experiment_id = experiment.experiment_id, run_name=f'test_{method}'):\n",
    "        mlflow.log_param(f\"n_samples test\", batch_size_te*n_batch_te)\n",
    "        \n",
    "# Get run that was just created and its ID to use when tracking\n",
    "client = mlflow.tracking.MlflowClient() # Create a MlflowClient object\n",
    "runs = client.search_runs(experiment.experiment_id)\n",
    "mlflow_run_id_test = [r.info.run_id for r in runs if r.info.run_name==f'test_{method}'][0]\n",
    "\n",
    "total_val_loss = test_model(method=method, model=model, epoch=ix_epoch, loss_function=criterion, \n",
    "                             batch_size=batch_size_te, n_batch=n_batch_te,\n",
    "                             n_timesteps_in=ts_len, n_timesteps_out=len_preds, n_feats_in=len(feature_set)-len(remove_bands), n_feats_out=output_dim, \n",
    "                             remove_band=remove_bands, feature_set=feature_set, \n",
    "                             experiment=experiment, split='test', start_batch=start_batch, client=client, run_id=mlflow_run_id, checkpoint_folder=checkpoint_folder,\n",
    "                             sample_type=sample_type, exp=exp_te, cp_idx=(0,1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mlflow_run_id_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m mlflow\u001b[38;5;241m.\u001b[39mend_run(mlflow_run_id)\n\u001b[1;32m      2\u001b[0m mlflow\u001b[38;5;241m.\u001b[39mend_run(mlflow_run_id_val)\n\u001b[0;32m----> 3\u001b[0m mlflow\u001b[38;5;241m.\u001b[39mend_run(\u001b[43mmlflow_run_id_test\u001b[49m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'mlflow_run_id_test' is not defined"
     ]
    }
   ],
   "source": [
    "mlflow.end_run(mlflow_run_id)\n",
    "mlflow.end_run(mlflow_run_id_val)\n",
    "mlflow.end_run(mlflow_run_id_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": [
     "\"\"\"\n",
     "ZRL PHACT GROUP\n",
     "\n",
     "Pytorch Segmentation template\n",
     "\n",
     "Authors:    Michal Muszynski (MMU@zurich.ibm.com)\n",
     "            Jonas Weiss (jwe@zurich.ibm.com)\n",
     "            Thomas Brunschwiler (tbr@zurich.ibm.com)\n",
     "            Paolo Fraccaro (Paolo.Fraccaro@ibm.com)\n",
     "\n",
     "Created:    8-August-2022\n",
     "\"\"\""
    ]
   }
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
